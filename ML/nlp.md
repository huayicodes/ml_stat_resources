---
title: Natural Language Processing
has_children: false
parent: Machine Learning
nav_order: 5
---


## Introductory tutorials

- [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)

   Written by an insight alumnus. Begins with the simplest NLP method that could work, and then moves on to more nuanced solutions, such as feature engineering, word vectors, and deep learning.

- [A Review of Bert-based models](https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58)

   A blog post giving a review of BERT-based models and explains transformers at a high level.

- [What you need to know about the new 2019 Transformer Models](https://www.topbots.com/ai-nlp-research-big-language-models/)
  - XLNet: Generalized Autoregressive Pretraining for Language Understanding
  - ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
  - RoBERTa: A Robustly Optimized BERT Pretraining Approach

## Tools

- [*Spacy*: Industrial Strength Natural Language Processing](https://spacy.io/)

   A very convenient python package with lots of pre-trained models for NLP

- [ðŸ¤— transformers: State-of-the-art NLP for everyone:](https://huggingface.co/transformers/index.html)

   ðŸ¤— Transformers (formerly known as *pytorch-transformers* and *pytorch-pretrained-bert*) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pre-trained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.
